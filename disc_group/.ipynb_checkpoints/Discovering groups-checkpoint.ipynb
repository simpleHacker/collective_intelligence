{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedparser: This module makes it easy to get the title, links, and entries from any RSS or Atom feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Collect the data and process them for ML tasks\n",
    " generatefeedvector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import re\n",
    "\n",
    "# Returns title and dictionary of word counts for an RSS feed\n",
    "def getwordcounts(url):\n",
    "  # Parse the feed\n",
    "  d=feedparser.parse(url)\n",
    "  wc={}\n",
    "\n",
    "  # Loop over all the entries\n",
    "  for e in d.entries:\n",
    "    if 'summary' in e: \n",
    "        summary=e.summary\n",
    "    else:\n",
    "        summary=e.description\n",
    "\n",
    "    # Extract a list of words\n",
    "    words=getwords(e.title+' '+summary)\n",
    "    for word in words:\n",
    "      wc.setdefault(word,0)\n",
    "      wc[word]+=1\n",
    "  return d.feed.title,wc\n",
    "\n",
    "def getwords(html):\n",
    "  # Remove all the HTML tags\n",
    "  txt=re.compile(r'<[^>]+>').sub('',html)\n",
    "\n",
    "  # Split words by all non-alpha characters\n",
    "  words=re.compile(r'[^A-Z^a-z]+').split(txt)\n",
    "\n",
    "  # Convert to lowercase\n",
    "  return [word.lower(  ) for word in words if word!='']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the code loops over every line in feedlist.txt and generates the word counts for each blog, as well as the number of blogs each word appeared in (apcount). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "apcount={}\n",
    "wordcounts={}\n",
    "feedlist=[]\n",
    "for feedurl in file('feedlist.txt'):\n",
    "  feedlist.append(feedurl)\n",
    "  title,wc=getwordcounts(feedurl)\n",
    "  wordcounts[title]=wc\n",
    "  for word,count in wc.items(  ):\n",
    "    apcount.setdefault(word,0)\n",
    "    if count>1:\n",
    "      apcount[word]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "filter rare occur words and stoping words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21524"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(apcount.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist=[]\n",
    "for w,bc in apcount.items(  ):\n",
    "    frac=float(bc)/len(feedlist)\n",
    "    if frac>0.1 and frac<0.4 and len(w) > 2:\n",
    "        wordlist.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "big matrix of all the word counts for each of the blogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('blogdata.txt','w') as out:\n",
    "    out.write('Blog')\n",
    "    for word in wordlist:\n",
    "        out.write('\\t%s' % word)\n",
    "    out.write('\\n')\n",
    "    for blog,wc in wordcounts.items():\n",
    "  #deal with unicode outside the ascii range\n",
    "        blog = blog.encode('ascii','ignore')\n",
    "        out.write(blog)\n",
    "    for word in wordlist:\n",
    "        if word in wc:\n",
    "            out.write('\\t%d' % wc[word])\n",
    "        else:\n",
    "            out.write('\\t0')\n",
    "    out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readfile(filename):\n",
    "  lines=[line for line in file(filename)]\n",
    "\n",
    "  # First line is the column titles\n",
    "  colnames=lines[0].strip(  ).split('\\t')[1:]\n",
    "  rownames=[]\n",
    "  data=[]\n",
    "  for line in lines[1:]:\n",
    "    p=line.strip(  ).split('\\t')\n",
    "    # First column in each row is the rowname\n",
    "    rownames.append(p[0])\n",
    "    # The data for this row is the remainder of the row\n",
    "    data.append([float(x) for x in p[1:]])\n",
    "  return rownames,colnames,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some blogs contain more entries or much longer entries than others, and will thus contain more words overall. The Pearson correlation will correct for this, since it really tries to determine how well two sets of data fit onto a straight line\n",
    "The Pearson correlation code for this module will take two lists of numbers and return their correlation score: \n",
    "\n",
    "is a measure of the linear correlation between two variables X and Y. It has a value between +1 and −1, where 1 is total positive linear correlation, 0 is no linear correlation, and −1 is total negative linear correlation. \n",
    "r=r_{xy}={\\frac {n\\sum x_{i}y_{i}-\\sum x_{i}\\sum y_{i}}{{\\sqrt {n\\sum x_{i}^{2}-(\\sum x_{i})^{2}}}~{\\sqrt {n\\sum y_{i}^{2}-(\\sum y_{i})^{2}}}}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def pearson(v1,v2):\n",
    "  # Simple sums\n",
    "  sum1=sum(v1)\n",
    "  sum2=sum(v2)\n",
    "\n",
    "  # Sums of the squares\n",
    "  sum1Sq=sum([pow(v,2) for v in v1])\n",
    "  sum2Sq=sum([pow(v,2) for v in v2])\n",
    "\n",
    "  # Sum of the products\n",
    "  pSum=sum([v1[i]*v2[i] for i in range(len(v1))])\n",
    "\n",
    "  # Calculate r (Pearson score)\n",
    "  num=pSum-(sum1*sum2/len(v1))\n",
    "  den=sqrt((sum1Sq - pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1)))\n",
    "  if den==0: return 0\n",
    "\n",
    "  return 1.0-num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bicluster that has all of these properties, which you'll use to represent the hierarchical tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bicluster:\n",
    "  def __init__(self,vec,left=None,right=None,distance=0.0,id=None):\n",
    "    self.left=left\n",
    "    self.right=right\n",
    "    self.vec=vec\n",
    "    self.id=id\n",
    "    self.distance=distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real optimization is not record distance, it is record the intermediate sum result of each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hcluster(rows,distance=pearson):\n",
    "  distances={}\n",
    "  currentclustid=-1\n",
    "\n",
    "  # Clusters are initially just the rows\n",
    "  clust=[bicluster(rows[i],id=i) for i in range(len(rows))]\n",
    "\n",
    "  while len(clust)>1:\n",
    "    lowestpair=(0,1)\n",
    "    closest=distance(clust[0].vec,clust[1].vec)\n",
    "\n",
    "    # loop through every pair looking for the smallest distance\n",
    "    for i in range(len(clust)):\n",
    "      for j in range(i+1,len(clust)):\n",
    "        # distances is the cache of distance calculations\n",
    "        if (clust[i].id,clust[j].id) not in distances:\n",
    "          distances[(clust[i].id,clust[j].id)]=distance(clust[i].vec,clust[j].vec)\n",
    "\n",
    "        d=distances[(clust[i].id,clust[j].id)]\n",
    "\n",
    "        if d<closest:\n",
    "          closest=d\n",
    "          lowestpair=(i,j)\n",
    "\n",
    "    # calculate the average of the two clusters\n",
    "    mergevec=[\n",
    "    (clust[lowestpair[0]].vec[i]+clust[lowestpair[1]].vec[i])/2.0\n",
    "    for i in range(len(clust[0].vec))]\n",
    "\n",
    "    # create the new cluster\n",
    "    newcluster=bicluster(mergevec,left=clust[lowestpair[0]],\n",
    "                         right=clust[lowestpair[1]],\n",
    "                         distance=closest,id=currentclustid)\n",
    "\n",
    "    # cluster ids that weren't in the original set are negative\n",
    "    currentclustid-=1\n",
    "    del clust[lowestpair[1]]\n",
    "    del clust[lowestpair[0]]\n",
    "    clust.append(newcluster)\n",
    "\n",
    "  return clust[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blognames,words,data=readfile('blogdata.txt')\n",
    "clust=hcluster(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
